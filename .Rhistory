# Convert house_rules to lowercase
abnb$house_rules <- tolower(abnb$house_rules)
# Count listings that mention allowing pets
pets_allowed <- sum(str_detect(abnb$house_rules, "pets allowed"))
# Count listings that mention not allowing pets
no_pets <- sum(str_detect(abnb$house_rules, "no pets"))
cat("Number of listings that allow pets:", pets_allowed, "\n")
cat("Number of listings that do not allow pets:", no_pets, "\n")
library(tidyverse)
library(vroom)
library(readr)
library(stringr)
abnb <- read_csv("/Users/tylersmith/Development/Projects/doterra/Airbnb_Open_Data.csv")
# Replace spaces with underscores in column names
names(abnb) <- gsub(" ", "_", names(abnb))
# Convert house_rules to lowercase
abnb$house_rules <- tolower(abnb$house_rules)
# Count listings that mention allowing pets
pets_allowed <- sum(str_detect(abnb$house_rules, "pets"))
library(tidyverse)
library(vroom)
library(readr)
library(stringr)
abnb <- read_csv("/Users/tylersmith/Development/Projects/doterra/Airbnb_Open_Data.csv")
# Replace spaces with underscores in column names
names(abnb) <- gsub(" ", "_", names(abnb))
# Convert house_rules to lowercase
abnb$house_rules <- tolower(abnb$house_rules)
# Count listings that mention allowing pets
pets_allowed <- sum(str_detect(abnb$house_rules, "pets"))
pets_allowed
library(tidyverse)
library(vroom)
library(readr)
library(stringr)
abnb <- read_csv("/Users/tylersmith/Development/Projects/doterra/Airbnb_Open_Data.csv")
# Replace spaces with underscores in column names
names(abnb) <- gsub(" ", "_", names(abnb))
# Filter rows where 'house_rules' mentions 'pets'
pets <- abnb %>%
filter(str_detect(house_rules, "pet"))
# View the filtered data
head(pets)
view(pets)
library(tidyverse)
library(vroom)
library(readr)
library(stringr)
abnb <- read_csv("/Users/tylersmith/Development/Projects/doterra/Airbnb_Open_Data.csv")
# Replace spaces with underscores in column names
names(abnb) <- gsub(" ", "_", names(abnb))
# Filter rows where 'house_rules' mentions 'pets'
pets <- abnb %>%
filter(str_detect(house_rules, "no pets"))
# View the filtered data
head(pets)
# Filter rows where 'house_rules' mentions 'pets'
no_pets <- abnb %>%
filter(str_detect(house_rules, "no pets"))
view(no_pets)
install.packages('doParallel')
library(readr)
nfl_suspensions_data <- read_csv("Downloads/nfl-suspensions-data.csv")
View(nfl_suspensions_data)
library(tidvyerse)
nfl_suspensions <- read_csv("Downloads/nfl-suspensions-data.csv")
suspensions_per_year <- nfl_suspensions %>%
group_by(year) %>%
summarise(count = n())
print(suspensions_per_year)
getwd()
library(tidvyerse)
library(tidvyerse)
install.packages('tidyverser')
install.packages('tidyverse')
install.packages("tidyverse")
library(tidvyerse)
library(tidyverse)
library(vroom)
library(readr)
library(stringr)
abnb <- read_csv("/Users/tylersmith/Development/Projects/doterra/Airbnb_Open_Data.csv")
# Replace spaces with underscores in column names
names(abnb) <- gsub(" ", "_", names(abnb))
abnb <- abnb %>%
select(id, host_id, host_id, host_name, host_identity_verified, neighbourhood_group, neighbourhood, lat, long, room_type,
Construction_year, price) %>%
filter(host_identity_verified == 'verified')
# Transform the price column and convert Construction_year to factor
abnb <- abnb %>%
mutate(price = str_replace_all(price, "\\$", "") %>%    # Replace dollar signs
str_replace_all(",", "") %>%               # Replace commas
as.numeric()) %>%
mutate(Construction_year = as.factor(Construction_year))
# Transform the price column
abnb <- abnb %>%
mutate(price = str_replace_all(price, "\\$", "") %>%    # Replace dollar signs
str_replace_all(",", "") %>%               # Replace commas
as.numeric())
# Count NA's in columns
na_count <- colSums(is.na(abnb))
abnb <- abnb %>%
drop_na(neighbourhood_group, neighbourhood, lat, long)
# Check to make sure that the NA's are removed from the groups that I wanted
sum(is.na(abnb$neighbourhood_group))
sum(is.na(abnb$neighbourhood))
sum(is.na(abnb$lat))
sum(is.na(abnb$long))
unique(abnb$neighbourhood_group)
# Change misspelled values in neighbourhood_group to make sure they match the real value
abnb <- abnb %>%
mutate(neighbourhood_group = case_when(
neighbourhood_group == "brookln" ~ "Brooklyn",
neighbourhood_group == "manhatan" ~ "Manhattan",
TRUE ~ neighbourhood_group # This makes sure that other values are unchanged
))
unique(abnb$neighbourhood_group)
view(abnb)
view(abnb)
view(abnb2)
library(readr)
nfl_suspensions_data <- read_csv("Development/Projects/nfl-suspensions-data.csv")
View(nfl_suspensions_data)
nfl_suspensions <- read_csv("Development/Projects/nfl-suspensions-data.csv")
nfl_suspensions
nfl_suspensions %>%
filter(year == 2012)
library(tidyverse)
nfl_suspensions %>%
filter(year == 2012)
view(nfl_suspensions %>%
filter(year == 2012))
len(nfl_suspensions %>%
filter(year == 2012))
filter(year == 2012))
length(nfl_suspensions %>%
filter(year == 2012))
nfl_suspensions %>%
filter(year == 2012)
library(readr)
nfl_suspensions_data <- read_csv("Development/Projects/nfl-suspensions-data.csv")
View(nfl_suspensions_data)
library(tidyverse)
nfl_suspensions <- read_csv("Development/Projects/nfl-suspensions-data.csv")
view(nfl_suspensions)
all_years <- tibble(Year = 1986:2014)
nfl_data_complete <- all_years %>%
left_join(nfl_data, by = "Year") %>%
mutate(Suspensions = ifelse(is.na(Suspensions), 0, Suspensions))
nfl_data_complete <- all_years %>%
left_join(nfl_suspensions, by = "Year") %>%
mutate(Suspensions = ifelse(is.na(Suspensions), 0, Suspensions))
nfl_data_complete <- all_years %>%
left_join(nfl_suspensions, by = "year") %>%
mutate(Suspensions = ifelse(is.na(Suspensions), 0, Suspensions))
library(tidyverse)
nfl_suspensions <- read_csv("Development/Projects/nfl-suspensions-data.csv")
all_years <- tibble(Year = 1986:2015)
nfl_data_complete <- all_years %>%
left_join(nfl_suspensions, by = c("Year" = "Year")) %>%
mutate(Suspensions = ifelse(is.na(Suspensions), 0, Suspensions))
library(tidyverse)
nfl_suspensions <- read_csv("Development/Projects/nfl-suspensions-data.csv")
all_years <- tibble(Year = 1986:2015)
nfl_data_complete <- all_years %>%
left_join(nfl_suspensions, by = c("year" = "year")) %>%
mutate(Suspensions = ifelse(is.na(Suspensions), 0, Suspensions))
library(tidyverse)
nfl_suspensions <- read_csv("Development/Projects/nfl-suspensions-data.csv")
all_years <- tibble(year = 1986:2015)
nfl_data_complete <- all_years %>%
left_join(nfl_suspensions, by = c("year" = "year")) %>%
mutate(Suspensions = ifelse(is.na(Suspensions), 0, Suspensions))
library(tidyverse)
nfl_suspensions <- read_csv("Development/Projects/nfl-suspensions-data.csv")
all_years <- tibble(Year = 1986:2015)
nfl_suspensions_2 <- all_years %>%
left_join(nfl_suspensions, by = "year") %>%
group_by(year) %>%
summarize(suspensions_count = n())
library(tidyverse)
nfl_suspensions <- read_csv("Development/Projects/nfl-suspensions-data.csv")
all_years <- tibble(year = 1986:2015)
nfl_suspensions_2 <- all_years %>%
left_join(nfl_suspensions, by = "Year") %>%
group_by(year) %>%
summarize(suspensions_count = n())
library(tidyverse)
nfl_suspensions <- read_csv("Development/Projects/nfl-suspensions-data.csv")
all_years <- tibble(year = 1986:2015)
nfl_suspensions_2 <- all_years %>%
left_join(nfl_suspensions, by = "year") %>%
group_by(year) %>%
summarize(suspensions_count = n())
write.csv(nfl_suspensions_2, "nfl_supsensions_2.csv", row.names = FALSE)
view(nfl_suspensions_2)
nfl_suspensions_updated <- nfl_suspensions %>%
left_join(nfl_suspensions_2, by = "year")
view(nfl_suspensions_updated)
view(nfl_data_updated)
nfl_data_updated <- nfl_data %>%
full_join(nfl_suspensions_2, by = "year")
nfl_suspensions_updated <- nfl_suspensions %>%
full_join(nfl_suspensions_2, by = "year")
nfl_suspensions_updated$games[is.na(nfl_suspensions_updated$games)] <- 0
unique(nfl_suspensions_updated$Year)
unique(nfl_suspensions_updated$year)
install.packages('Rserve')
library(Rserve)
Rserve(args=“–no-save”)
library(Rserve)
Rserve(args = "--no-save")
library(tidyverse)
library(tidymodels)
library(vroom)
library(embed) # for target encoding
library(ranger)
library(rpart)
library(discrim)
library(naivebayes)
library(kknn)
library(doParallel)
library(themis)
library(stacks)
library(kernlab)
library(keras)
library(tensorflow)
library(bonsai)
library(lightgbm)
library(dbarts)
train <- vroom("./training.csv", na=c("","NULL", "NA")) %>%
mutate(IsBadBuy=factor(IsBadBuy))
setwd("~/Desktop/STAT348/DontGetKicked")
train <- vroom("./training.csv", na=c("","NULL", "NA")) %>%
mutate(IsBadBuy=factor(IsBadBuy))
test <- vroom("./test.csv", na=c("", "NA", "NULL"))
# Recipe ------------------------------------------------------------------
my_recipe <- recipe(IsBadBuy ~ ., data = train) %>%
step_novel(all_nominal_predictors(), -all_outcomes()) %>%
step_unknown(all_nominal_predictors()) %>%
step_lencode_mixed(all_nominal_predictors(), outcome = vars(IsBadBuy)) %>%
step_impute_mean(all_numeric_predictors()) %>%
step_corr(all_numeric_predictors(), threshold = .7) %>%
step_zv() %>%
step_normalize(all_numeric_predictors())
knn_model <- nearest_neighbor(neighbors=tune()) %>% # set or tune
set_mode("classification") %>%
set_engine("kknn")
knn_wf <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(knn_model)
# cross validation
knn_tuning_grid <- grid_regular(neighbors(),
levels = 5)
knn_folds <- vfold_cv(train, v = 5, repeats = 1)
## Run the CV
CV_results <- knn_wf %>%
tune_grid(resamples = knn_folds,
grid = knn_tuning_grid,
metrics = metric_set(roc_auc))
knn_bestTune <- CV_results %>%
select_best("roc_auc")
# finalize workflow
final_knn_wf <- knn_wf %>%
finalize_workflow(knn_bestTune) %>%
fit(data = train)
knn_predictions <- final_wf %>%
predict(new_data = test, type = 'prob') %>%
bind_cols(test) %>%
rename(IsBadBuy = .pred_1) %>%
select(RefId, IsBadBuy)
knn_predictions <- final_knn_wf %>%
predict(new_data = test, type = 'prob') %>%
bind_cols(test) %>%
rename(IsBadBuy = .pred_1) %>%
select(RefId, IsBadBuy)
knn_predictions
vroom_write(knn_predictions, file = './knn.csv', delim = ",")
library(tidyverse)
library(tidymodels)
library(vroom)
library(embed) # for target encoding
library(ranger)
library(rpart)
library(discrim)
library(naivebayes)
library(kknn)
library(doParallel)
library(themis)
library(stacks)
library(kernlab)
library(keras)
library(tensorflow)
library(bonsai)
library(lightgbm)
library(dbarts)
train <- vroom("./training.csv", na=c("","NULL", "NA")) %>%
mutate(IsBadBuy=factor(IsBadBuy))
test <- vroom("./test.csv", na=c("", "NA", "NULL"))
# Recipe ------------------------------------------------------------------
my_recipe <- recipe(IsBadBuy ~ ., data = train) %>%
step_novel(all_nominal_predictors(), -all_outcomes()) %>%
step_unknown(all_nominal_predictors()) %>%
step_lencode_mixed(all_nominal_predictors(), outcome = vars(IsBadBuy)) %>%
step_impute_mean(all_numeric_predictors()) %>%
step_corr(all_numeric_predictors(), threshold = .7) %>%
step_zv() %>%
step_normalize(all_numeric_predictors())
nb_model <- naive_Bayes(Laplace=tune(), smoothness=tune()) %>%
set_mode('classification') %>%
set_engine('naivebayes')
nb_wf <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(nb_model)
# Tune smoothness and Laplace here
nb_tuning_grid <- grid_regular(Laplace(),
smoothness(),
levels = 5)
## Split data for CV
nb_folds <- vfold_cv(train, v = 5, repeats = 1)
## Run the CV
CV_results <- nb_wf %>%
tune_grid(resamples = nb_folds,
grid = nb_tuning_grid,
metrics = metric_set(roc_auc)) # f_meas, sens, recall, spec, precision, accuracy
## Find Best Tuning Parameters
nb_bestTune <- CV_results %>%
select_best("roc_auc")
## Finalize the Workflow & fit it
final_nb_wf <- nb_wf %>%
finalize_workflow(nb_bestTune) %>%
fit(data = train)
nb_predictions <- final_nb_wf %>%
predict(new_data = test, type = 'prob') %>%
bind_cols(test) %>%
rename(IsBadBuy = .pred_1) %>%
select(RefId, IsBadBuy)
nb_predictions
vroom_write(knn_predictions, file = './nb.csv', delim = ",")
nb_predictions
vroom_write(nb_predictions, file = './nb.csv', delim = ",")
pen_log_mod <- logistic_reg(mixture = tune(), penalty = tune()) %>%
set_engine('glmnet')
pen_log_wf <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(pen_log_mod)
pen_tuning_grid <- grid_regular(penalty(),
mixture(),
levels = 5) ## L^2 total tuning possibilities
folds <- vfold_cv(train, v = 5, repeats = 1)
cv_results <- tune_grid(pen_log_wf,
resamples = folds,
grid = pen_tuning_grid,
metrics = metric_set(roc_auc))
collect_metrics(cv_results)
best_tune <- select_best(cv_results, 'roc_auc')
final_wf <- pen_log_wf %>%
finalize_workflow(best_tune) %>%
fit(data = train)
folds <- vfold_cv(train, v = 5, repeats=2)
untunedModel <- control_stack_grid()
nb_models <- nb_wf %>%
tune_grid(resamples=folds,
grid=nb_tuning_grid,
metrics=metric_set(roc_auc),
control = untunedModel)
pen_log_models <- pen_log_wf %>%
tune_grid(resamples=folds,
grid=pen_tuning_grid,
metrics=metric_set(roc_auc),
control = untunedModel)
# Specify with models to include
my_stack <- stacks() %>%
add_candidates(pen_log_models) %>%
add_candidates(nb_models)
## Fit the stacked model
stack_mod <- my_stack %>%
blend_predictions() %>% # LASSO penalized regression meta-learner
fit_members() ## Fit the members to the dataset
predictions <- stack_mod %>%
predict(new_data = test,
type = "prob")
predictions
vroom_write(predictions, file = './stack_nb_pen_log.csv', delim = ",")
submission <- predictions %>%
mutate(Id = row_number()) %>%
rename("IsBadBuy" = ".pred_1")
submission
submission <- predictions %>%
mutate(Id = row_number()) %>%
rename("IsBadBuy" = ".pred_1") %>%
select(3,2)
submission
submission <- predictions %>%
mutate(Id = train$RefId) %>%
rename("IsBadBuy" = ".pred_1") %>%
select(3,2)
predictions <- stack_mod %>%
predict(new_data = test,
type = "prob")
predictions
submission <- predictions %>%
mutate(Id = row_number()) %>%
rename("Action" = ".pred_1") %>%
select(3,2)
submission
submission <- predictions %>%
mutate(Id = train$RefId) %>%
rename("IsBadBuy" = ".pred_1") %>%
select(3,2)
submission <- predictions %>%
mutate(RefId = row_number()) %>%
rename("IsBadBuy" = ".pred_1") %>%
select(3,2)
submission
submission <- predictions %>%
mutate(RefId = row_number() + 73014) %>%  # Adjust starting point for RefId
rename(IsBadBuy = .pred_1) %>%
select(RefId, IsBadBuy)
submission
vroom_write(predictions, file = './stack_nb_pen_log.csv', delim = ",")
submission
vroom_write(predictions, file = './stack_nb_pen_log.csv', delim = ",")
vroom_write(submission, file = './stack_nb_pen_log.csv', delim = ",")
submission <- predictions %>%
mutate(RefId = row_number() + 121721) %>%  # Adjust starting point for RefId to match Kaggle's expectation
rename(IsBadBuy = .pred_1) %>%
select(RefId, IsBadBuy)
submission
predictions <- stack_mod %>%
predict(new_data = test,
type = "prob")
predictions
library(tidyverse)
library(tidymodels)
library(vroom)
library(embed) # for target encoding
library(ranger)
library(rpart)
library(discrim)
library(naivebayes)
library(kknn)
library(doParallel)
library(themis)
library(stacks)
library(kernlab)
library(keras)
library(tensorflow)
library(bonsai)
library(lightgbm)
library(dbarts)
train <- vroom("./training.csv", na=c("","NULL", "NA")) %>%
mutate(IsBadBuy=factor(IsBadBuy))
test <- vroom("./test.csv", na=c("", "NA", "NULL"))
# Recipe ------------------------------------------------------------------
my_recipe <- recipe(IsBadBuy ~ ., data = train) %>%
step_novel(all_nominal_predictors(), -all_outcomes()) %>%
step_unknown(all_nominal_predictors()) %>%
step_lencode_mixed(all_nominal_predictors(), outcome = vars(IsBadBuy)) %>%
step_impute_mean(all_numeric_predictors()) %>%
step_corr(all_numeric_predictors(), threshold = .7) %>%
step_zv() %>%
step_normalize(all_numeric_predictors())
pen_log_mod <- logistic_reg(mixture = tune(), penalty = tune()) %>%
set_engine('glmnet')
pen_log_wf <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(pen_log_mod)
pen_tuning_grid <- grid_regular(penalty(),
mixture(),
levels = 5) ## L^2 total tuning possibilities
folds <- vfold_cv(train, v = 5, repeats = 1)
cv_results <- tune_grid(pen_log_wf,
resamples = folds,
grid = pen_tuning_grid,
metrics = metric_set(roc_auc))
collect_metrics(cv_results)
best_tune <- select_best(cv_results, 'roc_auc')
final_wf <- pen_log_wf %>%
finalize_workflow(best_tune) %>%
fit(data = train)
nb_model <- naive_Bayes(Laplace=tune(), smoothness=tune()) %>%
set_mode('classification') %>%
set_engine('naivebayes')
nb_wf <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(nb_model)
# Tune smoothness and Laplace here
nb_tuning_grid <- grid_regular(Laplace(),
smoothness(),
levels = 5)
## Split data for CV
nb_folds <- vfold_cv(train, v = 5, repeats = 1)
## Run the CV
CV_results <- nb_wf %>%
tune_grid(resamples = nb_folds,
grid = nb_tuning_grid,
metrics = metric_set(roc_auc)) # f_meas, sens, recall, spec, precision, accuracy
## Find Best Tuning Parameters
nb_bestTune <- CV_results %>%
select_best("roc_auc")
## Finalize the Workflow & fit it
final_nb_wf <- nb_wf %>%
finalize_workflow(nb_bestTune) %>%
fit(data = train)
folds <- vfold_cv(train, v = 5, repeats=2)
untunedModel <- control_stack_grid()
nb_models <- nb_wf %>%
tune_grid(resamples=folds,
grid=nb_tuning_grid,
metrics=metric_set(roc_auc),
control = untunedModel)
